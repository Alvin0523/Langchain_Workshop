{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title_header",
   "metadata": {},
   "source": [
    "# LangChain Workshop: Building an AI-Powered Chatbot\n",
    "### CCDS Tech for Good 2026 Hackathon\n",
    "\n",
    "**Workshop Goals:**\n",
    "- Understand LangChain fundamentals\n",
    "- Build conversational AI agents\n",
    "- Create production-ready chatbot classes\n",
    "- Master prompt engineering techniques\n",
    "\n",
    "**What you'll build:**\n",
    "- Basic chatbot with memory\n",
    "- AI companion with personality\n",
    "- Tool-using agents\n",
    "- Reusable chatbot framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "# 1. Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e6af05",
   "metadata": {},
   "source": [
    "## **uv : A Fast Python Management Setup**\n",
    "\n",
    "**[Official Docs](https://docs.astral.sh/uv/getting-started/installation/)** `Click this link to install`\n",
    "\n",
    "`uv` replaces standard tools (`pip`, `venv`, `virtualenv`) with a single, unified toolkit that is significantly faster.\n",
    "\n",
    "---\n",
    "\n",
    "### **Core Commands**\n",
    "\n",
    "| Action | Command | Description |\n",
    "| --- | --- | --- |\n",
    "| **Add Library** | `uv add <package>` | Installs the package and adds it to `pyproject.toml`. |\n",
    "| **View Graph** | `uv tree` | Visualizes the full dependency tree (parents & children). |\n",
    "| **Restore** | `uv sync` | **The only command needed after cloning.** Installs everything from `uv.lock`. |\n",
    "| **Run Script** | `uv run <script.py>` | Runs a script inside the managed environment automatically. |\n",
    "\n",
    "---\n",
    "\n",
    "### **Dependency Management Files**\n",
    "\n",
    "* **`pyproject.toml` ( The \"Wishlist\" )**\n",
    "* Lists the high-level packages you explicitly requested (e.g., `langchain`, `numpy`).\n",
    "* This is the file you edit if you want to change project settings.\n",
    "\n",
    "\n",
    "* **`uv.lock` ( The \"Snapshot\" )**\n",
    "* Contains the **exact** version and hash of every single package installed (including transitive dependencies).\n",
    "* Ensures that everyone working on the project has the exact same environment.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Generated File Hierarchy**\n",
    "\n",
    "When you initialize and use `uv`, your project structure will look like this:\n",
    "\n",
    "```text\n",
    "my_project/\n",
    "â”œâ”€â”€ .venv/               # [Hidden] The actual virtual environment containing binaries/libs.\n",
    "â”œâ”€â”€ .python-version      # Pins the specific Python version (e.g., 3.12).\n",
    "â”œâ”€â”€ pyproject.toml       # User-defined dependencies.\n",
    "â”œâ”€â”€ uv.lock              # Machine-generated freeze of all dependencies.\n",
    "â””â”€â”€ src/                 # (Recommended) Your source code folder.\n",
    "    â””â”€â”€ main.py\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Workflow for Cloning**\n",
    "\n",
    "`uv sync`\n",
    "\n",
    "This single command reads `uv.lock`, creates the `.venv`, and installs all locked dependencies to match the original environment perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_instructions",
   "metadata": {},
   "source": [
    "## API Key Configuration\n",
    "\n",
    "Create a `.env` file in your project directory:\n",
    "\n",
    "```\n",
    "AZURE_OPENAI_API_KEY=your_api_key_here\n",
    "AZURE_OPENAI_ENDPOINT=your_endpoint_here\n",
    "```\n",
    "\n",
    "**âš ï¸ Important**: Never commit your `.env` file to GitHub! Add it to `.gitignore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import langchain\n",
    "    import langchain_openai\n",
    "    from dotenv import load_dotenv\n",
    "    print(\"All packages installed successfully!\")\n",
    "    print(f\"Python version: {sys.version}\")\n",
    "    print(f\"LangChain version: {langchain.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"Missing package: {e}\")\n",
    "    print(\"Run: pip install langchain langchain-openai python-dotenv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basics_header",
   "metadata": {},
   "source": [
    "# 2. LangChain Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first_call_header",
   "metadata": {},
   "source": [
    "## 2.1 Your First LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first_call_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    deployment_name=\"gpt-5-nano\",\n",
    ")\n",
    "\n",
    "# Create messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
    "    HumanMessage(content=\"Explain what LangChain is in one sentence.\")\n",
    "]\n",
    "\n",
    "# Get response\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding_components",
   "metadata": {},
   "source": [
    "###  Understanding the Components\n",
    "\n",
    "**1. SystemMessage**: Defines the AI's role and behavior\n",
    "- Sets personality and expertise\n",
    "- Provides context and constraints\n",
    "- Remains consistent across conversation\n",
    "\n",
    "**2. HumanMessage**: User's input to the AI\n",
    "- The query or prompt\n",
    "- Changes with each interaction\n",
    "\n",
    "**3. Temperature**: Controls randomness (0-1)\n",
    "- `0.0` â†’ Deterministic, focused\n",
    "- `0.7` â†’ Balanced (recommended)\n",
    "- `1.0` â†’ Creative, varied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory_header",
   "metadata": {},
   "source": [
    "## 2.2 Adding Conversation Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory_basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Initialize LLM\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    deployment_name=\"gpt-5-nano\"\n",
    ")\n",
    "\n",
    "# Create memory buffer\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create conversation chain\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True  # Shows internal processing\n",
    ")\n",
    "\n",
    "# Multi-turn conversation\n",
    "print(\"Turn 1:\")\n",
    "response1 = conversation.predict(input=\"Hi! My name is Sarah.\")\n",
    "print(f\"AI: {response1}\\n\")\n",
    "\n",
    "print(\"Turn 2:\")\n",
    "response2 = conversation.predict(input=\"What's my name?\")  # AI remembers!\n",
    "print(f\"AI: {response2}\\n\")\n",
    "\n",
    "print(\"Turn 3:\")\n",
    "response3 = conversation.predict(input=\"What did we just talk about?\")\n",
    "print(f\"AI: {response3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory_types_header",
   "metadata": {},
   "source": [
    "## 2.3 Different Memory Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory_types_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationSummaryMemory\n",
    ")\n",
    "\n",
    "# 1. Buffer Memory - Stores ALL conversation history\n",
    "buffer_memory = ConversationBufferMemory()\n",
    "buffer_memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"Hello!\"})\n",
    "buffer_memory.save_context({\"input\": \"How are you?\"}, {\"output\": \"I'm doing well!\"})\n",
    "print(\"=== Buffer Memory (Stores Everything) ===\")\n",
    "print(buffer_memory.load_memory_variables({}))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 2. Window Memory - Only keeps last K interactions (RECOMMENDED)\n",
    "window_memory = ConversationBufferWindowMemory(k=2)  # Last 2 exchanges only\n",
    "window_memory.save_context({\"input\": \"Message 1\"}, {\"output\": \"Response 1\"})\n",
    "window_memory.save_context({\"input\": \"Message 2\"}, {\"output\": \"Response 2\"})\n",
    "window_memory.save_context({\"input\": \"Message 3\"}, {\"output\": \"Response 3\"})\n",
    "print(\"=== Window Memory (k=2, Most Efficient) ===\")\n",
    "print(window_memory.load_memory_variables({}))\n",
    "print(\"Note: Message 1 was forgotten!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 3. Summary Memory - Summarizes old conversations\n",
    "summary_memory = ConversationSummaryMemory(llm=llm)\n",
    "summary_memory.save_context(\n",
    "    {\"input\": \"Tell me about machine learning\"}, \n",
    "    {\"output\": \"Machine learning is a subset of AI that enables computers to learn from data without explicit programming...\"}\n",
    ")\n",
    "print(\"=== Summary Memory (Summarizes History) ===\")\n",
    "print(summary_memory.load_memory_variables({}))\n",
    "\n",
    "print(\"\\nðŸ’¡ Recommendation: Use ConversationBufferWindowMemory for your projects!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompts_header",
   "metadata": {},
   "source": [
    "## 2.4 Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompts_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Create a flexible prompt template\n",
    "template = \"\"\"\n",
    "You are a {personality} AI assistant specialized in {domain}.\n",
    "\n",
    "User's question: {question}\n",
    "\n",
    "Provide a {tone} response.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"personality\", \"domain\", \"question\", \"tone\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Create chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Example 1: Friendly tutor\n",
    "print(\"=== Friendly Tutor ===\")\n",
    "response = chain.predict(\n",
    "    personality=\"friendly and encouraging\",\n",
    "    domain=\"mathematics\",\n",
    "    question=\"What is the Pythagorean theorem?\",\n",
    "    tone=\"simple and easy to understand\"\n",
    ")\n",
    "print(response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Example 2: Professional advisor\n",
    "print(\"=== Professional Advisor ===\")\n",
    "response = chain.predict(\n",
    "    personality=\"professional and analytical\",\n",
    "    domain=\"career development\",\n",
    "    question=\"How do I prepare for a data science interview?\",\n",
    "    tone=\"structured and actionable\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chatbot_project_header",
   "metadata": {},
   "source": [
    "# 3. Building Your AI Companion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic_chatbot_header",
   "metadata": {},
   "source": [
    "## 3.1 Basic Chatbot with Personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_chatbot_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define chatbot personality\n",
    "chatbot_personality = \"\"\"\n",
    "You are Alex, a friendly and knowledgeable AI companion. You are:\n",
    "- Patient and understanding\n",
    "- Enthusiastic about learning and helping\n",
    "- Conversational and warm\n",
    "- Clear and concise in explanations\n",
    "\n",
    "Always respond in a friendly, natural way. Keep responses focused and helpful.\n",
    "\"\"\"\n",
    "\n",
    "# Create custom prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=f\"{chatbot_personality}\\n\\nConversation history:\\n{{history}}\\n\\nHuman: {{input}}\\nAlex:\"\n",
    ")\n",
    "\n",
    "# Initialize chatbot\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=5)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Test the chatbot\n",
    "print(\"Testing Alex the AI Companion:\\n\")\n",
    "print(\"You: Hi Alex! How are you today?\")\n",
    "print(f\"Alex: {conversation.predict(input='Hi Alex! How are you today?')}\\n\")\n",
    "\n",
    "print(\"You: Can you help me with my studies?\")\n",
    "print(f\"Alex: {conversation.predict(input='Can you help me with my studies?')}\\n\")\n",
    "\n",
    "print(\"You: What was the first thing I asked you?\")\n",
    "print(f\"Alex: {conversation.predict(input='What was the first thing I asked you?')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chatbot_class_header",
   "metadata": {},
   "source": [
    "## 3.2 Reusable Chatbot Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chatbot_class_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "\n",
    "class AICompanion:\n",
    "    \"\"\"\n",
    "    A flexible AI chatbot class that can be customized for different purposes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name, personality, domain=None, memory_window=5):\n",
    "        \"\"\"\n",
    "        Initialize the AI companion.\n",
    "        \n",
    "        Args:\n",
    "            name (str): Name of the AI companion\n",
    "            personality (str): Personality description\n",
    "            domain (str, optional): Area of expertise\n",
    "            memory_window (int): Number of exchanges to remember\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.personality = personality\n",
    "        self.domain = domain\n",
    "        \n",
    "        # Initialize LLM\n",
    "        self.llm = AzureChatOpenAI(\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            api_version=\"2024-02-15-preview\",\n",
    "            deployment_name=\"gpt-35-turbo\",\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Setup memory\n",
    "        self.memory = ConversationBufferWindowMemory(k=memory_window)\n",
    "        \n",
    "        # Create conversation chain\n",
    "        self._setup_conversation()\n",
    "    \n",
    "    def _setup_conversation(self):\n",
    "        \"\"\"Setup the conversation chain with custom prompt\"\"\"\n",
    "        \n",
    "        # Build system prompt\n",
    "        system_prompt = f\"You are {self.name}. {self.personality}\"\n",
    "        if self.domain:\n",
    "            system_prompt += f\"\\n\\nYou specialize in {self.domain}.\"\n",
    "        \n",
    "        system_prompt += \"\\n\\nProvide helpful, clear, and friendly responses.\"\n",
    "        \n",
    "        # Create prompt template\n",
    "        template = f\"{system_prompt}\\n\\nConversation history:\\n{{history}}\\n\\nHuman: {{input}}\\n{self.name}:\"\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"history\", \"input\"],\n",
    "            template=template\n",
    "        )\n",
    "        \n",
    "        # Create conversation chain\n",
    "        self.conversation = ConversationChain(\n",
    "            llm=self.llm,\n",
    "            memory=self.memory,\n",
    "            prompt=prompt,\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        \"\"\"\n",
    "        Send a message to the AI companion.\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): User's message\n",
    "        \n",
    "        Returns:\n",
    "            str: AI's response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.conversation.predict(input=user_input)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            return f\"Sorry, I encountered an error: {str(e)}\"\n",
    "    \n",
    "    def reset_memory(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.memory.clear()\n",
    "        print(f\"{self.name}'s memory has been cleared!\")\n",
    "    \n",
    "    def get_conversation_history(self):\n",
    "        \"\"\"Get the current conversation history\"\"\"\n",
    "        return self.memory.load_memory_variables({})\n",
    "\n",
    "# Example 1: Study Buddy\n",
    "print(\"=== Example 1: Study Buddy ===\")\n",
    "study_buddy = AICompanion(\n",
    "    name=\"StudyBot\",\n",
    "    personality=\"You are patient, encouraging, and great at explaining complex topics simply.\",\n",
    "    domain=\"mathematics and science\"\n",
    ")\n",
    "\n",
    "print(f\"User: Explain Newton's first law\")\n",
    "print(f\"StudyBot: {study_buddy.chat('Explain Newton\\'s first law')}\\n\")\n",
    "\n",
    "# Example 2: Wellness Coach\n",
    "print(\"\\n=== Example 2: Wellness Coach ===\")\n",
    "wellness_coach = AICompanion(\n",
    "    name=\"WellnessBot\",\n",
    "    personality=\"You are supportive, motivating, and focused on healthy habits.\",\n",
    "    domain=\"health and wellness\"\n",
    ")\n",
    "\n",
    "print(f\"User: Give me tips for better sleep\")\n",
    "print(f\"WellnessBot: {wellness_coach.chat('Give me tips for better sleep')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interactive_chatbot_header",
   "metadata": {},
   "source": [
    "## 3.3 Interactive Console Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive_chatbot_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_interactive_chatbot():\n",
    "    \"\"\"\n",
    "    Run an interactive chat session in the console.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Interactive AI Companion\")\n",
    "    print(\"Commands: 'quit' to exit, 'reset' to clear memory\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Create chatbot\n",
    "    bot = AICompanion(\n",
    "        name=\"Companion\",\n",
    "        personality=\"You are friendly, helpful, and conversational.\"\n",
    "    )\n",
    "    \n",
    "    print(\"Companion: Hi! I'm your AI companion. How can I help you today?\\n\")\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        # Check for commands\n",
    "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(\"\\nCompanion: Goodbye! Have a great day!\")\n",
    "            break\n",
    "        \n",
    "        if user_input.lower() == 'reset':\n",
    "            bot.reset_memory()\n",
    "            continue\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        # Get and display response\n",
    "        response = bot.chat(user_input)\n",
    "        print(f\"\\nCompanion: {response}\\n\")\n",
    "\n",
    "# Run the interactive chatbot\n",
    "# Uncomment the line below to start chatting!\n",
    "# run_interactive_chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agents_header",
   "metadata": {},
   "source": [
    "# 4. Advanced: Agents with Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agents_intro",
   "metadata": {},
   "source": [
    "## 4.1 Understanding Agents\n",
    "\n",
    "**Agents** are LLMs that can:\n",
    "- Use external tools\n",
    "- Make decisions about which tool to use\n",
    "- Chain multiple tool calls together\n",
    "\n",
    "Think of agents as \"AI that can DO things\" beyond just chatting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom_tools_header",
   "metadata": {},
   "source": [
    "## 4.2 Creating Custom Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom_tools_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, create_react_agent\n",
    "from langchain import hub\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Define tool functions\n",
    "def get_current_time(input_text=\"\"):\n",
    "    \"\"\"Get the current time\"\"\"\n",
    "    return datetime.now().strftime(\"%I:%M %p on %B %d, %Y\")\n",
    "\n",
    "def calculate_math(expression):\n",
    "    \"\"\"Calculate a mathematical expression\"\"\"\n",
    "    try:\n",
    "        # Safe evaluation of basic math\n",
    "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "        return f\"The result is: {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error calculating: {str(e)}\"\n",
    "\n",
    "def get_random_fact(category=\"general\"):\n",
    "    \"\"\"Get a random interesting fact\"\"\"\n",
    "    facts = {\n",
    "        \"science\": [\n",
    "            \"Water can boil and freeze at the same time in a phenomenon called the triple point.\",\n",
    "            \"A day on Venus is longer than its year.\",\n",
    "            \"Honey never spoils - archaeologists found 3000-year-old honey that's still edible!\"\n",
    "        ],\n",
    "        \"general\": [\n",
    "            \"Bananas are berries, but strawberries aren't!\",\n",
    "            \"The shortest war in history lasted 38-45 minutes.\",\n",
    "            \"An octopus has three hearts.\"\n",
    "        ]\n",
    "    }\n",
    "    return random.choice(facts.get(category, facts[\"general\"]))\n",
    "\n",
    "# Create Tool objects\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"GetTime\",\n",
    "        func=get_current_time,\n",
    "        description=\"Gets the current date and time. Use this when asked about the current time or date.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=calculate_math,\n",
    "        description=\"Calculates mathematical expressions. Input should be a valid Python math expression like '5 + 3' or '10 * 2'.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"FactGenerator\",\n",
    "        func=get_random_fact,\n",
    "        description=\"Provides interesting random facts. Input can be 'science' for science facts or leave empty for general facts.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize LLM\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Get the ReAct prompt\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "# Create agent\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# Test the agent\n",
    "print(\"=== Testing Agent with Tools ===\")\n",
    "print(\"\\nQuery 1: What time is it?\")\n",
    "result = agent_executor.invoke({\"input\": \"What time is it?\"})\n",
    "print(f\"Final Answer: {result['output']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"Query 2: Calculate 15 * 8 + 12\")\n",
    "result = agent_executor.invoke({\"input\": \"Calculate 15 * 8 + 12\"})\n",
    "print(f\"Final Answer: {result['output']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"Query 3: Tell me a science fact\")\n",
    "result = agent_executor.invoke({\"input\": \"Tell me an interesting science fact\"})\n",
    "print(f\"Final Answer: {result['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent_chatbot_header",
   "metadata": {},
   "source": [
    "## 4.3 Chatbot with Tools and Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agent_chatbot_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "class SmartChatbot:\n",
    "    \"\"\"\n",
    "    Advanced chatbot with tool usage capabilities and memory.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name, tools_list):\n",
    "        \"\"\"\n",
    "        Initialize smart chatbot with tools.\n",
    "        \n",
    "        Args:\n",
    "            name (str): Chatbot name\n",
    "            tools_list (list): List of Tool objects\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.tools = tools_list\n",
    "        \n",
    "        # Initialize LLM\n",
    "        self.llm = AzureChatOpenAI(\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            api_version=\"2024-02-15-preview\",\n",
    "            deployment_name=\"gpt-35-turbo\",\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Setup memory\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # Create agent\n",
    "        prompt = hub.pull(\"hwchase17/react\")\n",
    "        agent = create_react_agent(self.llm, self.tools, prompt)\n",
    "        \n",
    "        self.agent_executor = AgentExecutor(\n",
    "            agent=agent,\n",
    "            tools=self.tools,\n",
    "            memory=self.memory,\n",
    "            verbose=False,\n",
    "            handle_parsing_errors=True\n",
    "        )\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        \"\"\"\n",
    "        Chat with the smart chatbot.\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): User's message\n",
    "        \n",
    "        Returns:\n",
    "            str: Chatbot's response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = self.agent_executor.invoke({\"input\": user_input})\n",
    "            return result['output']\n",
    "        except Exception as e:\n",
    "            return f\"Sorry, I encountered an error: {str(e)}\"\n",
    "    \n",
    "    def list_tools(self):\n",
    "        \"\"\"List available tools\"\"\"\n",
    "        return [tool.name for tool in self.tools]\n",
    "\n",
    "# Create smart chatbot\n",
    "smart_bot = SmartChatbot(name=\"SmartBot\", tools_list=tools)\n",
    "\n",
    "print(f\"Available tools: {smart_bot.list_tools()}\\n\")\n",
    "\n",
    "# Test multi-turn conversation\n",
    "print(\"Turn 1:\")\n",
    "print(f\"User: What's the current time?\")\n",
    "print(f\"SmartBot: {smart_bot.chat('What is the current time?')}\\n\")\n",
    "\n",
    "print(\"Turn 2:\")\n",
    "print(f\"User: Calculate 25 * 4\")\n",
    "print(f\"SmartBot: {smart_bot.chat('Calculate 25 * 4')}\\n\")\n",
    "\n",
    "print(\"Turn 3:\")\n",
    "print(f\"User: What did you just calculate?\")\n",
    "print(f\"SmartBot: {smart_bot.chat('What did you just calculate for me?')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "production_header",
   "metadata": {},
   "source": [
    "# 5. Production-Ready Chatbot Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "production_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import Tool, AgentExecutor, create_react_agent\n",
    "from langchain import hub\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class ProductionChatbot:\n",
    "    \"\"\"\n",
    "    A production-ready chatbot framework with:\n",
    "    - Conversation memory\n",
    "    - Tool usage\n",
    "    - Error handling\n",
    "    - Logging\n",
    "    - Customizable personality\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initialize chatbot with configuration.\n",
    "        \n",
    "        Args:\n",
    "            config (dict): Configuration dictionary with keys:\n",
    "                - name: Chatbot name\n",
    "                - personality: Personality description\n",
    "                - domain: Area of expertise (optional)\n",
    "                - memory_window: Number of exchanges to remember\n",
    "                - temperature: LLM temperature (0-1)\n",
    "                - tools: List of Tool objects (optional)\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.name = config.get('name', 'Assistant')\n",
    "        self.personality = config.get('personality', 'You are a helpful AI assistant.')\n",
    "        self.domain = config.get('domain')\n",
    "        self.memory_window = config.get('memory_window', 5)\n",
    "        self.temperature = config.get('temperature', 0.7)\n",
    "        self.tools = config.get('tools', [])\n",
    "        \n",
    "        # Conversation log\n",
    "        self.conversation_log = []\n",
    "        \n",
    "        # Initialize LLM\n",
    "        self.llm = AzureChatOpenAI(\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            api_version=\"2024-02-15-preview\",\n",
    "            deployment_name=\"gpt-35-turbo\",\n",
    "            temperature=self.temperature\n",
    "        )\n",
    "        \n",
    "        # Setup memory\n",
    "        self.memory = ConversationBufferWindowMemory(\n",
    "            k=self.memory_window,\n",
    "            memory_key=\"chat_history\" if self.tools else \"history\",\n",
    "            return_messages=bool(self.tools)\n",
    "        )\n",
    "        \n",
    "        # Setup conversation or agent\n",
    "        if self.tools:\n",
    "            self._setup_agent()\n",
    "        else:\n",
    "            self._setup_conversation()\n",
    "    \n",
    "    def _setup_conversation(self):\n",
    "        \"\"\"Setup basic conversation chain\"\"\"\n",
    "        system_prompt = f\"You are {self.name}. {self.personality}\"\n",
    "        if self.domain:\n",
    "            system_prompt += f\" You specialize in {self.domain}.\"\n",
    "        \n",
    "        template = f\"{system_prompt}\\n\\nConversation history:\\n{{history}}\\n\\nHuman: {{input}}\\n{self.name}:\"\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"history\", \"input\"],\n",
    "            template=template\n",
    "        )\n",
    "        \n",
    "        self.chain = ConversationChain(\n",
    "            llm=self.llm,\n",
    "            memory=self.memory,\n",
    "            prompt=prompt,\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    def _setup_agent(self):\n",
    "        \"\"\"Setup agent with tools\"\"\"\n",
    "        prompt = hub.pull(\"hwchase17/react\")\n",
    "        agent = create_react_agent(self.llm, self.tools, prompt)\n",
    "        \n",
    "        self.agent_executor = AgentExecutor(\n",
    "            agent=agent,\n",
    "            tools=self.tools,\n",
    "            memory=self.memory,\n",
    "            verbose=False,\n",
    "            handle_parsing_errors=True\n",
    "        )\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        \"\"\"\n",
    "        Process user input and return response.\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): User's message\n",
    "        \n",
    "        Returns:\n",
    "            dict: Response dictionary with 'response', 'timestamp', 'success'\n",
    "        \"\"\"\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        try:\n",
    "            # Get response\n",
    "            if self.tools:\n",
    "                result = self.agent_executor.invoke({\"input\": user_input})\n",
    "                response = result['output']\n",
    "            else:\n",
    "                response = self.chain.predict(input=user_input)\n",
    "            \n",
    "            # Log conversation\n",
    "            self.conversation_log.append({\n",
    "                'timestamp': timestamp,\n",
    "                'user': user_input,\n",
    "                'assistant': response,\n",
    "                'success': True\n",
    "            })\n",
    "            \n",
    "            return {\n",
    "                'response': response,\n",
    "                'timestamp': timestamp,\n",
    "                'success': True\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error: {str(e)}\"\n",
    "            \n",
    "            # Log error\n",
    "            self.conversation_log.append({\n",
    "                'timestamp': timestamp,\n",
    "                'user': user_input,\n",
    "                'assistant': error_msg,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            })\n",
    "            \n",
    "            return {\n",
    "                'response': \"Sorry, I encountered an error processing your request.\",\n",
    "                'timestamp': timestamp,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset conversation memory\"\"\"\n",
    "        self.memory.clear()\n",
    "        self.conversation_log.append({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'action': 'memory_reset'\n",
    "        })\n",
    "    \n",
    "    def get_conversation_log(self):\n",
    "        \"\"\"Get full conversation log\"\"\"\n",
    "        return self.conversation_log\n",
    "    \n",
    "    def export_conversation(self, filename):\n",
    "        \"\"\"Export conversation log to JSON file\"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.conversation_log, f, indent=2)\n",
    "        print(f\"Conversation exported to {filename}\")\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get conversation statistics\"\"\"\n",
    "        total = len([log for log in self.conversation_log if 'user' in log])\n",
    "        successful = len([log for log in self.conversation_log if log.get('success')])\n",
    "        failed = total - successful\n",
    "        \n",
    "        return {\n",
    "            'total_exchanges': total,\n",
    "            'successful': successful,\n",
    "            'failed': failed,\n",
    "            'success_rate': f\"{(successful/total*100):.1f}%\" if total > 0 else \"0%\"\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "print(\"=== Creating Production Chatbot ===\")\n",
    "\n",
    "config = {\n",
    "    'name': 'HealthBot',\n",
    "    'personality': 'You are a supportive wellness companion focused on healthy habits.',\n",
    "    'domain': 'health and wellness',\n",
    "    'memory_window': 5,\n",
    "    'temperature': 0.7\n",
    "}\n",
    "\n",
    "chatbot = ProductionChatbot(config)\n",
    "\n",
    "# Test conversation\n",
    "print(\"\\nTest Conversation:\\n\")\n",
    "\n",
    "response1 = chatbot.chat(\"Hi! I want to improve my sleep.\")\n",
    "print(f\"User: Hi! I want to improve my sleep.\")\n",
    "print(f\"{chatbot.name}: {response1['response']}\\n\")\n",
    "\n",
    "response2 = chatbot.chat(\"What about exercise?\")\n",
    "print(f\"User: What about exercise?\")\n",
    "print(f\"{chatbot.name}: {response2['response']}\\n\")\n",
    "\n",
    "# Get stats\n",
    "print(\"\\nConversation Stats:\")\n",
    "print(json.dumps(chatbot.get_stats(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best_practices_header",
   "metadata": {},
   "source": [
    "# 6. Best Practices & Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best_practices_content",
   "metadata": {},
   "source": [
    "##  Key Best Practices\n",
    "\n",
    "### 1. **Memory Management**\n",
    "```python\n",
    "#  Good: Use window memory to control costs\n",
    "memory = ConversationBufferWindowMemory(k=5)\n",
    "\n",
    "#  Avoid: Unlimited buffer for long conversations\n",
    "# memory = ConversationBufferMemory()  # Can get expensive!\n",
    "```\n",
    "\n",
    "### 2. **Temperature Selection**\n",
    "- **0.0-0.3**: Factual tasks, consistent outputs\n",
    "- **0.7**: Balanced (recommended for most cases)\n",
    "- **0.9-1.0**: Creative writing, varied responses\n",
    "\n",
    "### 3. **Error Handling**\n",
    "```python\n",
    "try:\n",
    "    response = chatbot.chat(user_input)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    response = \"Sorry, something went wrong.\"\n",
    "```\n",
    "\n",
    "### 4. **Prompt Engineering**\n",
    "- Be specific and clear\n",
    "- Provide context and examples\n",
    "- Define personality and constraints\n",
    "- Keep system prompts focused\n",
    "\n",
    "### 5. **API Cost Optimization**\n",
    "- Limit conversation history\n",
    "- Use appropriate temperature\n",
    "- Cache common responses\n",
    "- Monitor token usage\n",
    "\n",
    "### 6. **Security**\n",
    "- Never commit API keys\n",
    "- Use environment variables\n",
    "- Validate user inputs\n",
    "- Implement rate limiting\n",
    "\n",
    "### 7. **Testing**\n",
    "```python\n",
    "test_cases = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What can you help me with?\",\n",
    "    \"Tell me about yourself\"\n",
    "]\n",
    "\n",
    "for test in test_cases:\n",
    "    response = chatbot.chat(test)\n",
    "    print(f\"Input: {test}\")\n",
    "    print(f\"Output: {response}\\n\")\n",
    "```\n",
    "\n",
    "### 8. **Code Organization**\n",
    "```\n",
    "project/\n",
    "â”œâ”€â”€ chatbot.py          # Main chatbot class\n",
    "â”œâ”€â”€ tools.py            # Custom tool definitions\n",
    "â”œâ”€â”€ config.py           # Configuration\n",
    "â”œâ”€â”€ .env                # API keys (gitignored!)\n",
    "```\n",
    "\n",
    "### 9. **Common Pitfalls to Avoid**\n",
    "- Storing sensitive data in prompts\n",
    "- Not handling API failures\n",
    "- Unlimited conversation history\n",
    "- Overly complex system prompts\n",
    "- Not testing edge cases\n",
    "\n",
    "### 10. **Deployment Checklist**\n",
    "- [ ] Environment variables configured\n",
    "- [ ] Error handling implemented\n",
    "- [ ] Memory limits set\n",
    "- [ ] Logging enabled\n",
    "- [ ] Rate limiting considered\n",
    "- [ ] API keys secured\n",
    "- [ ] Testing completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheatsheet_header",
   "metadata": {},
   "source": [
    "# 7. Quick Reference Cheat Sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheatsheet_content",
   "metadata": {},
   "source": [
    "## Essential Code Snippets\n",
    "\n",
    "### Initialize LLM\n",
    "```python\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    temperature=0.7\n",
    ")\n",
    "```\n",
    "\n",
    "### Create Memory\n",
    "```python\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=5)\n",
    "```\n",
    "\n",
    "### Simple Conversation\n",
    "```python\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "response = conversation.predict(input=\"Hello!\")\n",
    "```\n",
    "\n",
    "### Custom Prompt\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"You are {role}. {input}\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"role\", \"input\"],\n",
    "    template=template\n",
    ")\n",
    "```\n",
    "\n",
    "### Create Tool\n",
    "```python\n",
    "from langchain.agents import Tool\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"ToolName\",\n",
    "    func=your_function,\n",
    "    description=\"What the tool does\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Error Handling\n",
    "```python\n",
    "try:\n",
    "    response = chatbot.chat(user_input)\n",
    "except Exception as e:\n",
    "    response = f\"Error: {e}\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
